{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neural Network</h2>\n",
    "\n",
    "In this module, we learn to use Neural Network to solve classification and regression problems\n",
    "\n",
    "<h3>Classification</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Applied on the Credit Approval Data </h4>\n",
    "\n",
    "As usual, try the model on the credit approval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "crx = pd.read_csv('crx.data', header=None)\n",
    "crx.head()\n",
    "\n",
    "Y = np.zeros(crx.shape[0])           #create a vector of zeros with size = the data\n",
    "Y[crx[15]=='+'] = 1                  #when the actual target is +, Y is assigned 1\n",
    "crx[15] = Y  \n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(crx, crx[15]):\n",
    "    strat_train_set = crx.loc[train_index]\n",
    "    strat_test_set = crx.loc[test_index]\n",
    "    \n",
    "trainX = strat_train_set.loc[:,:14]\n",
    "trainY = strat_train_set.loc[:,15]\n",
    "trainX.shape, trainY.shape\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "num_cols = trainX.columns[(trainX.dtypes == np.int64) | (trainX.dtypes == np.float64)]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('standardize', StandardScaler())\n",
    "])\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#get a list of class columns\n",
    "cat_cols = trainX.columns[trainX.dtypes==object]\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='constant',fill_value='missing')),\n",
    "    ('encode', OneHotEncoder())\n",
    "])\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('numeric', num_pipeline, num_cols),\n",
    "    ('class', cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "trainX_prc = full_pipeline.fit_transform(trainX)\n",
    "\n",
    "testX = strat_test_set.loc[:,:14]\n",
    "testY = strat_test_set.loc[:,15]\n",
    "\n",
    "testX_prc = full_pipeline.transform(testX)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification, we use MLPClassifier.\n",
    "\n",
    "The architecture of the NN is decided by the hidden_layer_sizes hyperparameter. In short, this is a list of integer numbers, each number represent the number of hidden neuron in the corresponding layer. \n",
    "\n",
    "For example, \n",
    "\n",
    "hidden_layer_sizes=[10,20,30] \n",
    "\n",
    "represents a NN with three hidden layers, the first hidden layer has 10 neurons, the 2nd 20 neurons, and the last 30 neurons.\n",
    "\n",
    "NN is also trained iteratively, so you can also set max_iter to a high value to make sure the training converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9980657640232108\n",
      "0.8208092485549133\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "n_features = trainX_prc.shape[1] #get the number of input features\n",
    "mlp = MLPClassifier(hidden_layer_sizes=[n_features,n_features,n_features], max_iter=1000)\n",
    "\n",
    "mlp.fit(trainX_prc, trainY)\n",
    "print(mlp.score(trainX_prc, trainY))\n",
    "print(mlp.score(testX_prc, testY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the model is overfitting.\n",
    "\n",
    "Now let's finetune the NN. I'm just gonna train a few architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
       "                                     batch_size='auto', beta_1=0.9,\n",
       "                                     beta_2=0.999, early_stopping=False,\n",
       "                                     epsilon=1e-08, hidden_layer_sizes=(100,),\n",
       "                                     learning_rate='constant',\n",
       "                                     learning_rate_init=0.001, max_fun=15000,\n",
       "                                     max_iter=1000, momentum=0.9,\n",
       "                                     n_iter_no_change=10,\n",
       "                                     nesterovs_momentum=True, power_t=0.5,\n",
       "                                     random_state=None, shuffle=True,\n",
       "                                     solver='adam', tol=0.0001,\n",
       "                                     validation_fraction=0.1, verbose=False,\n",
       "                                     warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid=[{'alpha': [0.001, 0.01, 0.1, 1, 10],\n",
       "                          'hidden_layer_sizes': [[51, 51], [51, 51, 51],\n",
       "                                                 [25, 25], [25, 25, 25],\n",
       "                                                 [102, 102],\n",
       "                                                 [102, 102, 102]]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [{\n",
    "    'hidden_layer_sizes' : [[n_features,n_features],                       #two hidden layer with n_features neurons\n",
    "                            [n_features,n_features,n_features],            #three hidden layer with n_features neurons \n",
    "                            [n_features//2,n_features//2],                 #two hidden layer with n_features/2 neurons\n",
    "                            [n_features//2,n_features//2,n_features//2],   #three hidden layer with n_features/2 neurons\n",
    "                            [n_features*2,n_features*2],                   #two hidden layer with n_features*2 neurons\n",
    "                            [n_features*2,n_features*2,n_features*2]],     #three hidden layer with n_features*2 neurons\n",
    "    'alpha' : [0.001, 0.01, 0.1, 1, 10]                                    #regularization terms\n",
    "}]\n",
    "\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=3, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "grid_search.fit(trainX_prc,trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best training model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 10, 'hidden_layer_sizes': [102, 102]}\n",
      "0.8510328449164315\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8728323699421965"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_dt = grid_search.best_estimator_\n",
    "best_dt.score(testX_prc, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Summarize all Results</h4>\n",
    "\n",
    "Compared to other models so far:\n",
    "\n",
    "|Model|Training CV Accuracy| Testing Accuracy|\n",
    "|-----|--------------------|-----------------|\n",
    "|No Regularization|0.834|0.850|\n",
    "|L2 Regularization|0.857|0.861|\n",
    "|L1 Regularization|0.861|0.861|\n",
    "|ENet Regularization|0.863|0.861|\n",
    "|L1 Linear SVM|0.851|0.861|\n",
    "|L2 Linear SVM|0.853|0.873|\n",
    "|Kernel SVM|0.872|0.867|\n",
    "|Decision Tree|0.858|0.867|\n",
    "|NN|0.851|0.873|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regression, training NN is essentially the same. The only difference is that we use MLPRegressor instead of MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
